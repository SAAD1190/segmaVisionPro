from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, BlipForConditionalGeneration, BlipProcessor
from PIL import Image
import torch
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class PromptGenerator:
    
    def __init__(self, image_directory="./data/", images_extensions=['jpg', 'jpeg', 'png']):
        # Initialize models and processors
        self.models = {
            "blip": BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device),
            "blip2": BlipForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-6.7b-coco").to(device),
            "vit_gpt2": VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning").to(device),
            "vilt": VisionEncoderDecoderModel.from_pretrained("dandelin/vilt-b32-finetuned-coco").to(device),
            "simvlm": VisionEncoderDecoderModel.from_pretrained("microsoft/SimVLM-base").to(device)
        }
        
        # Initialize processors for each model
        self.processors = {
            "blip": BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base"),
            "blip2": BlipProcessor.from_pretrained("Salesforce/blip2-opt-6.7b-coco"),
            "vit_gpt2": ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning"),
            "vilt": ViTImageProcessor.from_pretrained("dandelin/vilt-b32-finetuned-coco"),
            "simvlm": ViTImageProcessor.from_pretrained("microsoft/SimVLM-base")
        }

        # Initialize tokenizers where applicable
        self.tokenizers = {
            "vit_gpt2": AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning"),
            "vilt": AutoTokenizer.from_pretrained("dandelin/vilt-b32-finetuned-coco"),
            "simvlm": AutoTokenizer.from_pretrained("microsoft/SimVLM-base")
        }

        self.image_directory = image_directory
        self.images_extensions = images_extensions
        self.prompt_pool = {}

    def generate_prompts(self, max_length=15, num_prompts_per_model=5):
        images = []
        image_names = []

        # Load images from the directory
        for image_name in os.listdir(self.image_directory):
            if image_name.split('.')[-1].lower() in self.images_extensions:
                image_path = os.path.join(self.image_directory, image_name)
                try:
                    i_image = Image.open(image_path)
                    if i_image.mode != "RGB":
                        i_image = i_image.convert(mode="RGB")
                    images.append(i_image)
                    image_names.append(image_name)
                except Exception as e:
                    print(f"Error opening image {image_name}: {e}")
                    continue

        if not images:
            print("No valid images found!")
            return

        # Generate prompts from each model
        for idx, image_name in enumerate(image_names):
            image_prompts = {}
            print(f"Generating prompts for {image_name}...")
            
            # Process image pixel values for each model
            for model_name, model in self.models.items():
                processor = self.processors[model_name]
                
                # Handle BLIP2 and BLIP models separately as they use BlipProcessor
                if model_name in ["blip", "blip2"]:
                    inputs = processor(images=images[idx:idx+1], return_tensors="pt").to(device)
                    pixel_values = inputs['pixel_values']
                else:
                    # For other models using ViTImageProcessor
                    pixel_values = processor(images=images[idx:idx+1], return_tensors="pt").pixel_values.to(device)

                prompts = []
                for _ in range(num_prompts_per_model):
                    output_ids = model.generate(
                        pixel_values,
                        max_length=max_length,
                        num_beams=1,  # Disable beam search
                        do_sample=True,  # Enable sampling
                        top_k=50,  # Top-k sampling
                        temperature=1.0  # Sampling temperature
                    )
                    
                    if model_name in ["blip", "blip2"]:
                        pred = processor.decode(output_ids[0], skip_special_tokens=True).strip()
                    else:
                        # For the other models using AutoTokenizer
                        pred = self.tokenizers[model_name].decode(output_ids[0], skip_special_tokens=True).strip()

                    prompts.append(pred)

                # Store the prompts generated by the current model
                image_prompts[model_name] = prompts

            # Add the prompts for the current image to the prompt pool
            self.prompt_pool[image_name] = image_prompts

        return self.prompt_pool